{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h17-N44jGJLk"
   },
   "source": [
    "Last update: 17th January, 2024\n",
    "\n",
    "**Important Note:** You should have first uploaded the **entire** folder for Problem Sheet 1 to your Google Drive/ARC cluster. Note the path that leads to this directory!\n",
    "\n",
    "You can then\n",
    "- open this notebook with Google Colab through your drive (you are advised against opening this file directly in Google Colab), or\n",
    "- follow the official ARC guide to open this notebook in ARC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tACLV729446Y"
   },
   "source": [
    "# Setup for Google Colab\n",
    "\n",
    "> **Important: You should complete the following setups everytime if you have restarted a new runtime in Google Colab (e.g. switching from CPU to GPU and vice versa).**\n",
    "\n",
    "**If you are running this notebook in Google Colab,** run the following cell to mount your Google drive to this notebook. The Google Drive will then ask for your permission to write in your drive. **You can skip this step if you are running this notebook in ARC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3R1HTcYs5Dyo"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQgQbx4j5IsW"
   },
   "source": [
    "Run the following `cd` command to make sure that this notebook now runs in the desired repository we want. If you are doing this question in Google Colab, you should run\n",
    "```\n",
    "%cd drive/MyDrive/{replace this with your path of directory}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bADWGwcg5upG"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/{\"replace this with your path of directory\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOutdYbF9nde"
   },
   "source": [
    "# Setup for ARC\n",
    "You should follow section 6 to start an interactive session in ARC with GPU access. In particular, **before running any of the codes**, you should load the following modules:\n",
    "\n",
    "```\n",
    "module load PyTorch/1.11.0-foss-2022a-CUDA-11.7.0\n",
    "module load h5py/3.7.0-foss-2022a\n",
    "module load tqdm/4.64.0-GCCcore-11.3.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kek3zOI040qo"
   },
   "source": [
    "\n",
    "# Question 3/4: Classifying digits in the MNIST dataset\n",
    "\n",
    "We would like to classify the digits from the MNIST dataset, with the data in the form $(x_i, y_i)$ with $x_i \\in \\mathbb{R}^d$ ($d=28\\times 28 = 784$) being the image and $y_i \\in \\mathbb{R}^K$ ($K=10$) being the labels. Recall the architecture of a shallow (single-layer) fully-connected neural network: $f(x,\\theta)$ for $\\theta=(C,W,b^{(1)},b^{(2)})$, defined iteratively as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    Z &= Wx + b^{(1)} \\in \\mathbb{R}^{d_H} \\\\\n",
    "    H_i &= \\sigma(Z_i) \\\\\n",
    "    f(x,\\theta) &= F_{\\mathsf{softmax}}(CH + b^{(2)}) \\in \\mathbb{R}^K,\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $\\sigma(\\cdot)$ is an activation function to be specified, and that $d_H$ is the number of hidden units.\n",
    "\n",
    "Before this, let us setup the environments for you. This is done by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z-yJWkQgHLvm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from random import randint\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWF802kxJGA1"
   },
   "source": [
    "# Load the class MNIST dataset.\n",
    "Run the cell below to convert the dataset to the familiar `np.array` format.\n",
    "\n",
    "> **If you are running the notebook in Google Colab and that the above setup fails**, you can upload the dataset (`MNISTdata.hdf5`) directly to the file explorer. Please consult [this tutorial](https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7) from the `towardsdatascience.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8II5MnNJHfEH"
   },
   "outputs": [],
   "source": [
    "MNIST_data = h5py.File('./MNISTdata.hdf5', 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:] )\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:,0]))\n",
    "x_test = np.float32( MNIST_data['x_test'][:] )\n",
    "y_test = np.int32( np.array( MNIST_data['y_test'][:,0]))\n",
    "\n",
    "MNIST_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6r_Wtezhgde"
   },
   "source": [
    "# Readme first: speeding up your computation by matrix multiplication and broadcasting\n",
    "\n",
    "## Matrix Multiplication\n",
    "\n",
    "The product of a matrix `W` and a vector `x` could be written as `W @ x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1673489852377,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": -480
    },
    "id": "X_v8JevJhgO4",
    "outputId": "59f07c20-4c4f-4316-9945-c91405791c0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2],[1,1]])\n",
    "x = np.array([2,3])\n",
    "W @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hGkrZGbAhAG"
   },
   "source": [
    "How about evaluating multiple matrix-vector multiplications? We may do that by using matrix-matrix multiplication. For instance, if we want to evaluate `W @ x1` and `W @ x2` for matrix `W` and vectors `x1, x2`, we could\n",
    "- form `X = np.array([x1, x2]).T`,\n",
    "- evaluate `W @ X`, then\n",
    "- `W @ x1 = (W @ X).T[0]` and\n",
    "- `W @ x2 = (W @ X).T[1]`.\n",
    "\n",
    "**Warning:** In doing this, we need to be careful with our convention for dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1673489854322,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": -480
    },
    "id": "wgxM3JyNAfPY",
    "outputId": "3de93773-43db-4e95-a363-62e897c10e99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2],[1,3]])\n",
    "x1 = np.array([2,3])\n",
    "x2 = np.array([1,1])\n",
    "X = np.array([x1,x2]).T\n",
    "all(W @ x1 == (W @ X).T[0]), all(W @ x2 == (W @ X).T[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kCxeiFKCNzZ"
   },
   "source": [
    "The reason for mentioning this is that, computing a matrix-matrix multiplication is much faster than looping many matrix-vector multiplication. This is often called *vectorisation*, and could significantly speed up the computation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3nFbbKtEUj-"
   },
   "source": [
    "## Broadcasting\n",
    "But how about computing `Wx + b` for multiple inputs `x = x1, x2, ...`? The `numpy` addition is capable of adding a matrix and a vector (as an 1-d array) together. But what does that even mean? Let us try it out with the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1673489857186,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": -480
    },
    "id": "gPKrYfYPETti",
    "outputId": "037c94d0-c1d7-4c81-981b-5ddc4704fd9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 5],\n",
       "       [3, 6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([[1,2],[1,3]])\n",
    "b = np.array([2,3])\n",
    "C + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nshieGOvHqpF"
   },
   "source": [
    "You will recognise that what addition of a matrix and an 1-d array really does is to compute the following\n",
    "$$C + b = C + \\begin{bmatrix} \\leftarrow & b & \\rightarrow \\\\ \\leftarrow & b & \\rightarrow \\\\ & \\vdots & \\\\ \\leftarrow & b & \\rightarrow \\end{bmatrix}.$$\n",
    "\n",
    "A better illustration is provided in the [official `numpy` documentation](https://numpy.org/doc/stable/user/basics.broadcasting.html):\n",
    "\n",
    "![Broadcasting](https://numpy.org/doc/stable/_images/broadcasting_2.png)\n",
    "\n",
    "This powerful feature is very useful in avoiding for loops when computing `Wx+b` for multiple inputs. **A caveat would be to be careful with the direction of broadcasting!** From the above figure, we see that broadcasting updates rows instead of columns (as mathematician could have expected!) This is due to historical reasons that computer scientists believed that working with rows are more natural...\n",
    "\n",
    "So what do we do if we want to compute `Wx + b` for multiple inputs `x = x1, x2, ...`, assuming `b` is an 1D array? We know that the rows of `(W @ X).T` (where `X = np.array([x1,x2,...]).T`) are the products `W @ x1, W @ x2, ...`, so we know that the rows of `(W @ X).T + b` are our desired inputs `W @ x1 + b, W @ x2 + b, ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1673489862872,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": -480
    },
    "id": "QAG7QEoEPGoc",
    "outputId": "dfa7ccdb-591a-432d-e2a1-be0bd20cc2ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.array([[1,2],[1,3]])\n",
    "x1 = np.array([2,3])\n",
    "x2 = np.array([1,1])\n",
    "b = np.array([.5, 1.5])\n",
    "X = np.array([x1,x2]).T\n",
    "all(W @ x1 + b == (W @ X).T[0] + b), all(W @ x2 + b == (W @ X).T[1] + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4KMHSngNug3"
   },
   "source": [
    "## I am super confused with the transpose!\n",
    "So do I, and there isn't much we can do. Computer scientist would therefore instead adopt the following convention when writing out the neural network\n",
    "\n",
    "$$f(x;C,W,b^{(1)},b^{(2)}) = F_{\\mathsf{softmax}} (C \\sigma.(xW+b^{(1)}) + b^{(2)}),$$\n",
    "\n",
    "assuming $x, b^{(1)}, b^{(2)}$ are **ROW** vectors. I personally would like to stick with the math convention. To the end, you probably don't need to worry about this, as computer scientists have implemented neural networks in `PyTorch` anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-BDB60SnI0RD"
   },
   "source": [
    "## Computing outer product\n",
    "We note that in performing backpropagation, we need to compute outer product of the two vectors (as 1D array), sav `v` and `w`. This is taken care of by the `np.outer` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1673489862873,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": -480
    },
    "id": "5HyQ98NnIu2b",
    "outputId": "26afa993-a93e-4e7a-e9bd-ee3b251c405f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [4, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([1,2])\n",
    "w = np.array([2,3])\n",
    "np.outer(v,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85lRMRCyHLGt"
   },
   "source": [
    "# Question 3 Part (b) - Implementing NN without PyTorch\n",
    "\n",
    "By the time of reading this notebook, you should have completed **part (a)** of the problem sheet, and read through pages 28-37 of the lecture slides. In this part, you are asked to implement and train a fully-connected neural network with one hidden layer. You will first do this from scratch, before doing this again with PyTorch in question 4.\n",
    "\n",
    "A neural network is defined as a `class` in Python. We will assume that you are familiar with working with class. If you need help, consult the following [W3school tutorial](https://www.w3schools.com/python/python_classes.asp).\n",
    "\n",
    "Read the instructions in the problem sheet to implement and train a shallow neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "29janzrzPjws"
   },
   "outputs": [],
   "source": [
    "class myShallowNetwork:\n",
    "  def __init__(self, num_inputs=784, num_outputs=10, num_hidden_units=200, sw=1, sb=1):\n",
    "    self.num_inputs = num_inputs\n",
    "    self.num_hidden_units = num_hidden_units\n",
    "    self.num_outputs = num_outputs\n",
    "\n",
    "    ### TODO (1): initialise the weights and biases. Your solutions goes here.\n",
    "    #self.W1 = np.full((self.num_hidden_units, self.num_inputs), sw,  dtype=np.float64) # Weights matrix W\n",
    "    #self.W2 = np.full((self.num_outputs, self.num_hidden_units), sw,  dtype=np.float64) # Weights matrix C \n",
    "    \n",
    "    self.W1 = np.random.randn(num_hidden_units,num_inputs) / np.sqrt(num_inputs) # Random Init of W\n",
    "    self.W2 = np.random.randn(num_outputs,num_hidden_units) / np.sqrt(num_hidden_units) # Random Init of C\n",
    "    self.b1 = np.zeros((num_hidden_units, 1))\n",
    "    self.b2 = np.zeros((num_outputs, 1))\n",
    "    \n",
    "\n",
    "  ### TODO (2): implement the hidden_function, softmax_function and hidden_function_grad.\n",
    "  def hidden_function(self,z):\n",
    "    # Sigmoid function \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "  def softmax_function(self, z):\n",
    "    e_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    return e_z / e_z.sum(axis=0, keepdims=True)\n",
    "\n",
    "  def hidden_function_grad(self,z):\n",
    "    # Find the gradient of sigmoid function\n",
    "    return self.hidden_function(z) * (1 - self.hidden_function(z))\n",
    "\n",
    "\n",
    "\n",
    "  ### TODO (3): implement the forward function (return two dimensional array)\n",
    "  def forward(self,x):\n",
    "    \n",
    "    # Reshape x \n",
    "    if x.ndim == 1: \n",
    "        Z = self.W1 @ x.reshape(-1, 1) + self.b1\n",
    "    else: \n",
    "        Z = self.W1 @ x + self.b1\n",
    "        \n",
    "    \n",
    "    # Compute Z, H, U, f and Rho\n",
    "    H = self.hidden_function(Z)\n",
    "    U = self.W2 @ H + self.b2\n",
    "    \n",
    "    f = self.softmax_function(U)\n",
    "\n",
    "    return f, Z, H\n",
    "\n",
    "\n",
    "  ### TODO (4): implement the backward function\n",
    "  def backward(self,x,y): \n",
    "    \n",
    "    # Run Forward Algorithm \n",
    "    f, Z, H = self.forward(x) \n",
    "    \n",
    "    #creating one hot encoded matrix (2D) \n",
    "    encoded_array = np.zeros((self.num_outputs, y.size))\n",
    "    encoded_array[y, np.arange(y.size),] = 1.0\n",
    "    \n",
    "    # Find gradients - need to check\n",
    "    dU = f - encoded_array\n",
    "    db2 = dU.reshape(-1,1)\n",
    "    dW2 = dU @ H.T # np.outer(dU, H.T)\n",
    "    delta = self.W2.T @ db2 # or dH\n",
    "    db1 = delta * self.hidden_function_grad(Z)\n",
    "    dW1 =  db1 @ x.T # np.outer(db1, x)  \n",
    "    \n",
    "    return dW1, dW2, db1, db2\n",
    "\n",
    "\n",
    "\n",
    "  def train(self, x_train, y_train, LR=.01, num_epochs=2):\n",
    "    time1 = time.time()\n",
    "\n",
    "    ### TODO (5): implement the train function\n",
    "    for epochs in range(num_epochs):\n",
    "\n",
    "        # Section: implement the SGD\n",
    "        for n in tqdm(range(10000)): #Has to be len(x_train)\n",
    "            \n",
    "          # randomly select a data point (x, y)\n",
    "          rand_index = randint(0,len(x_train)-1)\n",
    "            \n",
    "          x = x_train[rand_index] #.reshape(self.num_inputs, -1)\n",
    "          y = y_train[rand_index]  \n",
    "\n",
    "          # compute gradient with respect to cross-entropy loss\n",
    "          dW1, dW2, db1, db2 = self.backward(x, y)\n",
    "        \n",
    "          # update weights and biases\n",
    "          self.W2 -= LR * dW2\n",
    "          self.b2 -= LR * db2\n",
    "          self.W1 -= LR * dW1\n",
    "          self.b1 -= LR * db1\n",
    "\n",
    "        \n",
    "        # Section: At the end of each epoch, compute the training accuracy\n",
    "\n",
    "        # compute the probability vectors for each training inputs\n",
    "        # It should be emphasised that p is a 2D array containing all predicted probability as vectors\n",
    "        p, _, _ = self.forward(x_train.T)\n",
    "\n",
    "        # compute the prediction - you don't need to do a for loop here!\n",
    "        y_pred = np.argmax(p, axis = 0)\n",
    "\n",
    "        # compute the number of correct predictions - you don't need to do a for loop here!\n",
    "        total_correct = np.sum(y_pred == y_train)\n",
    "\n",
    "        print(f\"Train accuracy: {total_correct/len(x_train)}\")\n",
    "        ### End of TODO (5)\n",
    "\n",
    "\n",
    "    time2 = time.time()\n",
    "    print(f\"Training time is {time2-time1}s.\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  ### TODO (6): implement the predict function (hint: take inspiration from how you compute your training accuracies)\n",
    "  def predict(self, x_test):\n",
    "    p, _, _ = self.forward(x_test.reshape(784, len(x_test)))\n",
    "    y_pred = np.argmax(p, axis = 0)\n",
    "    return y_pred\n",
    "  ### End of TODO (6)\n",
    "\n",
    "  def test(self, x_test, y_test, plot_confusion=False):\n",
    "    ### TODO (7): implement the test function (hint: take inspiration from how you compute your training accuracies)\n",
    "    y_pred = self.predict(x_test.reshape(784, len(x_test)))\n",
    "    total_correct = np.sum(y_pred == y_test)\n",
    "    accuracy = total_correct/len(y_test)\n",
    "    ### End of TODO (7)\n",
    "\n",
    "    if plot_confusion:\n",
    "      ### TODO (8): plot the confusion matrix\n",
    "      ### (Hint: a suggestion is to use the np.histogram2d to count the number of different pairs, then use sns.heatmap to visualise the histogram)\n",
    "      \"your answer goes here\"\n",
    "      pass\n",
    "      ### End of TODO (8)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hv6IknmQ1qF"
   },
   "source": [
    "Congratulations in completing the To-dos. Before we actually train the neural network for classifying MNIST digits, let us compare our computed gradients with its finite-difference approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1673802738574,
     "user": {
      "displayName": "CHSamuelLam Official",
      "userId": "14638164322242343101"
     },
     "user_tz": 0
    },
    "id": "4FR6fPOvQkh9",
    "outputId": "a16b5a62-9d22-45f8-b907-1603630e7b37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute error of the derivative is [9.97175528e-08]\n"
     ]
    }
   ],
   "source": [
    "my_SNN = myShallowNetwork()\n",
    "my_SNN_new = myShallowNetwork()\n",
    "\n",
    "# initialise our new network such that it has same parameters as the original network\n",
    "my_SNN_new.W1 = my_SNN.W1.copy()\n",
    "my_SNN_new.W2 = my_SNN.W2.copy()\n",
    "my_SNN_new.b1 = my_SNN.b1.copy()\n",
    "my_SNN_new.b2 = my_SNN.b2.copy()\n",
    "\n",
    "# now perturb our parameters by a little bit,\n",
    "# here we demonstrate an entry for the bias for the hidden layer\n",
    "k = randint(0, my_SNN.num_hidden_units - 1)\n",
    "epsilon = 1e-9\n",
    "my_SNN_new.b1[k] += epsilon\n",
    "\n",
    "# we may then compute an estimate of the derivative, and compare with the derivative computed from backpropagation\n",
    "sample = randint(0,len(x_train)-1)\n",
    "x_input = x_train[sample].reshape(my_SNN.num_inputs, -1) # Used to be [:], also swapped order on differential\n",
    "y_input = y_train[sample]\n",
    "\n",
    "differential = ((-np.log(my_SNN_new.forward(x_input)[0][y_input][0])) - (-np.log(my_SNN.forward(x_input)[0][y_input][0])))/epsilon\n",
    "\n",
    "\n",
    "grad_W, grad_C, grad_b1, grad_b2 = my_SNN.backward(x_input, y_input)\n",
    "print(f\"absolute error of the derivative is {np.abs(differential - grad_b1[k])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m60CjQBaqAZf"
   },
   "source": [
    "You may repeat the above procedure for different partial derivatives. Once you are convinced that the algorithm works, you can run the following cell to see how good our implementation of shallow neural network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bdQ9vrZhp8bk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10000/10000 [00:06<00:00, 1662.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10000/10000 [00:06<00:00, 1622.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9009166666666667\n",
      "Training time is 13.135960102081299s.\n"
     ]
    }
   ],
   "source": [
    "my_SNN = myShallowNetwork()\n",
    "my_SNN.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "iYPjAvM_2UyF"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 7840000 into shape (784,784)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [297]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmy_SNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_confusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [293]\u001b[0m, in \u001b[0;36mmyShallowNetwork.test\u001b[0;34m(self, x_test, y_test, plot_confusion)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_test, y_test, plot_confusion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    124\u001b[0m   \u001b[38;5;66;03m### TODO (7): implement the test function (hint: take inspiration from how you compute your training accuracies)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m   total_correct \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y_pred \u001b[38;5;241m==\u001b[39m y_test)\n\u001b[1;32m    127\u001b[0m   accuracy \u001b[38;5;241m=\u001b[39m total_correct\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_test)\n",
      "Input \u001b[0;32mIn [293]\u001b[0m, in \u001b[0;36mmyShallowNetwork.predict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_test):\n\u001b[0;32m--> 118\u001b[0m   p, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[43mx_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(p, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m y_pred\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 7840000 into shape (784,784)"
     ]
    }
   ],
   "source": [
    "my_SNN.test(x_test, y_test, plot_confusion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxoZdQJ8V4UR"
   },
   "source": [
    "# Question 4 - Implement NN with PyTorch\n",
    "\n",
    "The question is designed for you to not only get used to the syntax of PyTorch, but also how to navigate with Python files in Google Colab and use the (free) GPU resources in Google Colab/ARC. Follow the following steps to complete your setup.\n",
    "\n",
    "1. Make sure that the demonstration file `PyTorch_MultiLayer_MNIST.py` is in the same directory as this notebook in your Google Drive / ARC storage, as well as the dataset `MNISTdata.hdf5`.\n",
    "\n",
    "> You can jump to step 4 if you are using ARC. Complete the following if you are using Google Colab.\n",
    "\n",
    "2. Now go to `Runtime` and select `Change runtime type`. Once you are done, click save.\n",
    "\n",
    "3. Since you have change your runtime, you have to setup your Google Colab. This involve mounting your Google drive and `cd`-ing to the correct directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrA4Bf8r6g0I"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNbdR9nm6g0J"
   },
   "outputs": [],
   "source": [
    "%cd drive/MyDrive/{\"replace this with your path of directory\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLyhDJmA6fYt"
   },
   "source": [
    "4. Now run the following cell to enable `autoreload` which **allows code changes to be effective immediately**. Please <font color=red>**always**</font> run the code cell below each time before you attempt this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oh7X-9I-cb6-"
   },
   "outputs": [],
   "source": [
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2S2gwn1fGse"
   },
   "source": [
    "5. Now run the following cell to run the demo file `PyTorch_MultiLayer_MNIST.py`. Notice how the training of the neural network is accelerated with the use of the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcSCfzcdgKeR"
   },
   "outputs": [],
   "source": [
    "%run ./PyTorch_MultiLayer_MNIST.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usQG24Daiohz"
   },
   "source": [
    "**Important:** Remember to change the runtime type back to `None` to save your GPU credits!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPPWMKyII82pmDy0vTPA//R",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
